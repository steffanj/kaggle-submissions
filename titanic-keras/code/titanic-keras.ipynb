{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Introduction \nThis kernel is based on my more general Titanic kernel which can be found [here][1], and aims to explore the use of neural networks (NNs) with the keras library for python. Data exploration is performed in the other kernel. \n\n## Preprocessing of training data\nExplanations for the preprocessing steps can be found [here][1].\n  [1]: https://www.kaggle.com/steffanj/titanic/titanic-preprocessing-eda-and-ml-in-python",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# First, let's load the training data to see what we're dealing with. \n# We will import the file to a pandas DataFrame:\nimport pandas as pd\ntrain_data = pd.read_csv('../input/train.csv')\n\n# Let's set the index of our DataFrame to the PassengerId column, \n# since that column is assigned arbitrarily (but uniquely) and thus should \n# not be regarded a real 'feature' of the data:\ntrain_data.set_index(['PassengerId'], inplace=True)\n\n# Use regex to extract the titles (Mr., Miss. etc.) of the passengers in \n# order to use as a categorical/nominal feature:\nimport re\nimport numpy as np\npatt = re.compile('\\s(\\S+\\.)') # 1 whitespace character followed by several \n# non-whitespace characters followed by a dot\ntitles = np.array([re.search(patt, i)[1] for i in train_data['Name'].values])\n\n# Include the titles as a new feature 'Title' in the DataFrame, \n# and drop the 'Name' feature:\ntrain_data = train_data.assign(Title=titles)\ntrain_data = train_data.drop('Name', axis=1)\n\n# Regroup infrequently occuring titles:  \ntrain_data['Title'] = train_data['Title'].replace('Mlle.','Miss.')\ntrain_data['Title'] = train_data['Title'].replace('Ms.','Miss.')  \ntrain_data['Title'] = train_data['Title'].replace('Mme.','Mrs.')\ntrain_data['Title'] = train_data['Title'].replace(['Capt.','Col.','Major.'],'Army.')\ntrain_data['Title'] = train_data['Title'].replace(['Countess.','Don.','Jonkheer.','Lady.','Sir.'],'Noble.')\n\n# Drop the 'Ticket' column:\ntrain_data = train_data.drop('Ticket', axis=1)\n\n# Extract letters from cabin codes. Passengers without a cabin will \n# have entries of 'None':\ndef getCabinCat(cabin_code):\n\tif pd.isnull(cabin_code):\n\t\tcat = 'None' # Use a string so that it is clear that this is \n\t\t\t\t\t # a category on its own\n\telse:\n\t\tcat = cabin_code[0]\n\treturn cat\n\ncabin_cats = np.array([getCabinCat(cc) for cc in train_data['Cabin'].values])\n\n# Add this as a new 'Cabin_cat' feature to the DataFrame, remove the 'Cabin' feature:\ntrain_data = train_data.assign(Cabin_cat=cabin_cats)\ntrain_data = train_data.drop('Cabin', axis=1)\n\n# Encoding categorical features ('Sex', 'Embarked', 'Title' and 'Cabin_cat') in a \n# numerical format, using  sklearn.preprocessing.LabelEncoder and \n# sklearn.preprocessing.OneHotEncoder. Missing values/Nans will be imputed using a \n# nearest neighbor approach (see [here][1] for explanations). \n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Converting to numerical features\n# Sex feature\nle_sex = LabelEncoder()\nsex_numerical = le_sex.fit_transform(train_data['Sex'])\nsex_numerical_classes = le_sex.classes_\n\n# Title feature\nle_title = LabelEncoder()\ntitle_numerical = le_title.fit_transform(train_data['Title'])\ntitle_numerical_classes = le_title.classes_\n\n# Cabin_cat feature\nle_cabin_cat = LabelEncoder()\ncabin_cat_numerical = le_cabin_cat.fit_transform(train_data['Cabin_cat'])\ncabin_cat_numerical_classes = le_cabin_cat.classes_\n\nfrom sklearn.preprocessing import OneHotEncoder\n# Sex feature\nenc_sex = OneHotEncoder(sparse=False)\nsex_onehot = enc_sex.fit_transform(sex_numerical.reshape(-1,1))\n\n# Title feature\nenc_title = OneHotEncoder(sparse=False)\ntitle_onehot = enc_title.fit_transform(title_numerical.reshape(-1,1))\n\n# Cabin_cat feature\nenc_cabin_cat = OneHotEncoder(sparse=False)\ncabin_cat_onehot = enc_cabin_cat.fit_transform(cabin_cat_numerical.reshape(-1,1))\n\ndef pdAssignWithOHLabel(df, column, onehot_labeled, class_labels):\n\tto_assign = {}\n\tfor c_idx, label in enumerate(class_labels):\n\t\tto_assign[column+'_'+label] = onehot_labeled[:,c_idx]\n\tdf = df.assign(**to_assign)\n\treturn df\n\n# Sex feature\ntrain_data = pdAssignWithOHLabel(train_data, 'Sex', \n\t\t\t\t\t\t\t\t sex_onehot, sex_numerical_classes)\ntrain_data = train_data.drop('Sex',axis=1)\n\n# Title feature\ntrain_data = pdAssignWithOHLabel(train_data, 'Title', \n\t\t\t\t\t\t\t\t title_onehot, title_numerical_classes)\ntrain_data = train_data.drop('Title',axis=1)\n\n# Cabin_cat feature\ntrain_data = pdAssignWithOHLabel(train_data, 'Cabin_cat', \n\t\t\t\t\t\t\tcabin_cat_onehot, cabin_cat_numerical_classes)\ntrain_data = train_data.drop('Cabin_cat',axis=1)\n\n# Set outliers in 'Fare' column to less extreme values\nmu = train_data['Fare'].mean()\nsd = train_data['Fare'].std()\nrow_mask = train_data['Fare']>mu+5*sd\ntrain_data.set_value(row_mask, 'Fare', mu+5*sd);\n\n# Standard scaling of all features except the 'Embarked' and 'Age' feature, \n# because both need to be imputed:\nfrom sklearn.preprocessing import StandardScaler\nsc_tmp = StandardScaler()\ntmp_scaled = train_data.copy().drop(['Embarked','Age','Survived'], axis=1) # create a copy of the data\ntmp_scaled = pd.DataFrame(sc_tmp.fit_transform(tmp_scaled),columns=tmp_scaled.columns, index=tmp_scaled.index)\n\n# Add the non-scaled features to this temporary DataFrame\ntmp_scaled = tmp_scaled.assign(Survived=train_data['Survived'])\ntmp_scaled = tmp_scaled.assign(Embarked=train_data['Embarked'])\ntmp_scaled = tmp_scaled.assign(Age=train_data['Age'])\n\nfrom sklearn.neighbors import KDTree\ntmp = tmp_scaled.copy().drop(['Survived','Age','Embarked'], axis=1).values\nrow_idx = pd.isnull(train_data['Embarked'])\ntree = KDTree(tmp)\ndist, ind = tree.query(tmp[[62, 830]], k=6) \n# The k nearest neighbors include the passenger itself, \n# so we specify k=6 to get the 5 nearest neighbors\n\n# Impute missing values in the 'Embarked' feature with an 'S':\ntrain_data.set_value([62, 830], 'Embarked', 'S');\n\n# Encode the values with numerical labels\nle_embarked = LabelEncoder()\nembarked_numerical = le_embarked.fit_transform(train_data['Embarked'])\nembarked_numerical_classes = le_embarked.classes_\n\n# One-Hot encoding\nenc_embarked = OneHotEncoder(sparse=False)\nembarked_onehot = enc_embarked.fit_transform(embarked_numerical.reshape(-1,1))\n\n# Add new features\ntrain_data = pdAssignWithOHLabel(train_data, 'Embarked', embarked_onehot, \n\t\t\t\t\t\t\t\t embarked_numerical_classes)\ntmp_scaled = pdAssignWithOHLabel(tmp_scaled, 'Embarked', embarked_onehot, \n\t\t\t\t\t\t\t\t embarked_numerical_classes)\n# Drop old feature\ntrain_data = train_data.drop('Embarked',axis=1)\ntmp_scaled = tmp_scaled.drop('Embarked',axis=1)\n\n# The new columns need to be standard-scaled:\nsc_tmp = StandardScaler()\ntmp = tmp_scaled[['Embarked_C', 'Embarked_Q', 'Embarked_S']].copy()\ntmp = pd.DataFrame(sc_tmp.fit_transform(tmp),columns=tmp.columns, index=tmp.index)\n\n# Drop the unscaled features from train_data \ntmp_scaled = tmp_scaled.drop(['Embarked_C', 'Embarked_Q', 'Embarked_S'], \n\t\t\t\t\t\t\t axis=1)\n\n# Assign the scaled features to train_data\ntmp_scaled = tmp_scaled.assign(Embarked_C=tmp['Embarked_C'])\ntmp_scaled = tmp_scaled.assign(Embarked_Q=tmp['Embarked_Q'])\ntmp_scaled = tmp_scaled.assign(Embarked_S=tmp['Embarked_S'])\n\n# Impute 'Age' feature:\ndef knnImpute(ori_arr, tmp_imp_arr, feature, k=6): # improved one\n\tfrom sklearn.neighbors import KDTree\n\trow_idx = ori_arr[pd.isnull(ori_arr[feature])].index.tolist()\n\ttree = KDTree(tmp_imp_arr) # tmp_arr is the array without \n\t\t\t\t\t\t   # the null-containing feature\n\tfor nan_v in row_idx:\n\t\tdist, ind = tree.query(tmp_imp_arr[nan_v,:].reshape(1,-1), k)\n\t\tnn_vals = ori_arr[feature].loc[ind[0][1:]]\n\t\timp_val = np.floor(np.nanmean(nn_vals))+0.5  \n\t\tori_arr.set_value(nan_v, feature, imp_val)\n\treturn ori_arr\n\ntmp_imp = tmp_scaled.copy().drop('Age', axis = 1).values\ntrain_data = knnImpute(train_data, tmp_imp, 'Age', 8)\n\n# Extract training data (without Survived feature) and class labels\ncolumns = ['Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male',\n\t   'Title_Army.', 'Title_Dr.', 'Title_Master.', 'Title_Miss.', 'Title_Mr.',\n\t   'Title_Mrs.', 'Title_Noble.', 'Title_Rev.', 'Cabin_cat_A',\n\t   'Cabin_cat_B', 'Cabin_cat_C', 'Cabin_cat_D', 'Cabin_cat_E',\n\t   'Cabin_cat_F', 'Cabin_cat_G', 'Cabin_cat_None', 'Cabin_cat_T',\n\t   'Embarked_C', 'Embarked_Q', 'Embarked_S']\n\ntrain_data_df = train_data # Keep the full train_data DataFrame for later usage\ntrain_labels = train_data['Survived'].values.ravel()\ntrain_data = train_data[columns].values\nsc_training = StandardScaler()\nsc_training = sc_training.fit(train_data)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# General import statements\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Keras \nWe'll work on implementing a keras classifier here. Let's start by making the necessary imports:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import StandardScaler",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now, let's define a function that will return a keras classifier built according to some parameters; we need this function to make a wrapper that will work in an sklearn pipeline:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def create_ann_model(layers, dropout_rate=None, loss='binary_crossentropy', optimizer='adam'):\n    model = Sequential()\n    for i, layer in enumerate(layers):\n        if len(layer) > 2:\n            model.add(Dense(layer[0], activation=layer[1], input_shape=layer[2]))\n        else:\n            model.add(Dense(layer[0], activation=layer[1]))\n            \n        if dropout_rate:\n            if 0 < i < len(layers):\n                model.add(Dropout(dropout_rate))    \n    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n    return model",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I have initially trained models using an exhaustive grid search with the following parameters: \n\n    layers = [\n        [(27,'relu',(train_data.shape[1],)), (5, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (10, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (15, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (27, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (20, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (15, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (10, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (5, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (20, 'relu'), (10, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (10, 'relu'), (10, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (10, 'relu'), (10, 'relu'), (10, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (10, 'relu'), (5, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (5, 'relu'), (5, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (14, 'relu'), (7, 'relu'), (4, 'relu'), (1, 'relu')]\n            ] dropout_rate = [None, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5] optimizer = ['adam','sgd','adamax'] epochs = [10, 30, 50]\n\nThe top performing classifiers using these parameters used 3 hidden layers, low or zero values for drop-out, and 30 or 50 epochs. A new gridsearch was performed using 3 - 5 hidden layers, various low values for drop-out, and with more epochs:\n\n    layers = [\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (20, 'relu'), (10, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (10, 'relu'), (10, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (10, 'relu'), (10, 'relu'), (10, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (10, 'relu'), (5, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (5, 'relu'), (5, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (14, 'relu'), (7, 'relu'), (4, 'relu'), (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (20, 'relu'), (10, 'relu'), (10, 'relu'), \n         (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (10, 'relu'), (10, 'relu'), (10, 'relu'), \n         (1, 'relu')],    \n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (20, 'relu'), (20, 'relu'), (20, 'relu'), \n         (1, 'relu')],    \n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (15, 'relu'), (10, 'relu'), (5, 'relu'), \n         (1, 'relu')],\n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (20, 'relu'), (10, 'relu'), (10, 'relu'), \n         (10, 'relu'), (1, 'relu')],    \n        [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (20, 'relu'), (20, 'relu'), (20, 'relu'), \n         (20, 'relu'), (1, 'relu')],        \n        [(27,'relu',(train_data.shape[1],)), (22, 'relu'), (16, 'relu'), (12, 'relu'), (8, 'relu'), \n         (4, 'relu'), (1, 'relu')],        \n            ]\n    dropout_rate = [None, 0.01, 0.03, 0.05, 0.07, 0.1, 0.12, 0.14, 0.16]\n    optimizer = ['adam','sgd','adamax']\n    epochs = [50, 100, 150]\n\nTraining all those models (in a cross-validated manner) takes a lot of time, which is why the code above was put in a text field. It is provided as reference. \n\nSome of the parameters that turned out to perform best are defined below and used in the computation of the final predictions.\n\n## Training models",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Model creation, compile and fit parameters\nlayers = [\n    [(27,'relu',(train_data.shape[1],)), (20, 'relu'), (20, 'relu'), (10, 'relu'), \n     (10, 'relu'), (1, 'relu')], \n        ]\ndropout_rate = [0.07]\noptimizer = ['adamax']\nepochs= [200]\n\n# Create a parameter grid of the above parameters\n# Even though only one parameter configuration will be checked here,\n# you could use the code below in conjunction with the more elaborate\n# grid parameters introduced above\nparam_grid = dict(clf__layers=layers, clf__dropout_rate=dropout_rate, \n                  clf__optimizer=optimizer, clf__epochs=epochs)\n\n# Create pipeline of standardscaler and classifier\nclf = KerasClassifier(build_fn=create_ann_model, verbose=0)\nsc = StandardScaler()\npipeline = Pipeline([('sc', sc),('clf', clf)])\n\n# Initiate and fit GridSearchCV\ngs = GridSearchCV(pipeline, param_grid, cv=None, n_jobs=-1, verbose=0)\ngs.fit(train_data, train_labels)\n\n# Save results of all tested classifiers\nfrom time import localtime, strftime\ntimestr = strftime(\"%Y-%m-%d-%H.%M\", localtime())\ncvstr = 'cv_scores_'+timestr+'.csv'\ncv = pd.DataFrame(gs.cv_results_) \ncv.to_csv(cvstr)\n\n# Retrain best classifier on all available training data\nbest_clf_retrained = gs.best_estimator_.fit(train_data, train_labels)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Show results, sorted on mean test score\ncv[['mean_test_score', 'param_clf__layers', 'param_clf__dropout_rate', \n    'param_clf__epochs', 'param_clf__optimizer']].sort_values('mean_test_score', ascending=False)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The best classifier from the results above will be used to make predictions on the test set.\n\n## Preprocessing test samples\nWe will assume for a moment that test set data only needs imputation on the 'Age' feature. Many operations from above need to be applied to the test set as well; loading the data, creating a DataFrame, One-Hot encoding the data, scaling the data:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Load test data\ntest_data = pd.read_csv('../input/test.csv')\n\n# Set DataFrame index\ntest_data.set_index(['PassengerId'], inplace=True)\n\n# Extract passenger titles\npatt = re.compile('\\s(\\S+\\.)') # 1 whitespace character followed by several \n# non-whitespace characters followed by a dot\ntitles_test = np.array([re.search(patt, i)[1] for i in test_data['Name'].values])\n# In the test data provided with the project (which will not be the test data on which\n# my models will be evaluated), the title of 'Dona.' was present, which was not encountered\n# in the training data. Passengers with the title of 'Dona.' will be merged with 'Noble.'. \n# New features that are encountered in the real test data but which are not accounted \n# for individually in this preprocessing stage will be assigned the most occuring value\n# for this feature (Mr.)\ntest_data = test_data.assign(Title=titles_test)\ntest_data = test_data.drop('Name', axis=1)\n# Regroup some Title values\ntest_data['Title'] = test_data['Title'].replace('Mlle.','Miss.')\ntest_data['Title'] = test_data['Title'].replace('Ms.','Miss.')  \ntest_data['Title'] = test_data['Title'].replace('Mme.','Mrs.')\ntest_data['Title'] = test_data['Title'].replace(['Capt.','Col.','Major.'],'Army.')\ntest_data['Title'] = test_data['Title'].replace(['Countess.','Don.', 'Dona.', 'Jonkheer.','Lady.','Sir.'],'Noble.')\n# Set unknown values for Title feature to Mr\nb_mask = test_data['Title'].isin(['Mr.','Sir.','Master.','Miss.','Mrs.','Lady.','Army.','Rev.', 'Noble.', 'Dr.'])\nb_mask = ~b_mask\nif b_mask.sum() > 0:\n    patt2 = re.compile('.*')\n    titles_test = test_data['Title'].copy()\n    titles_test = titles_test.loc[b_mask].replace(patt2,'Mr.')\n    test_data = test_data.drop('Title', axis=1)\n    test_data = test_data.assign(Title=titles_test)\n\n# Drop Ticket feature\ntest_data = test_data.drop('Ticket', axis=1)\n\n# Generate Cabin_cat feature\ncabin_cats = np.array([getCabinCat(cc) for cc in test_data['Cabin'].values])\ntest_data = test_data.assign(Cabin_cat=cabin_cats)\ntest_data = test_data.drop('Cabin', axis=1)\n    \n# Converting textual categorical features to numbers\nsex_numerical = le_sex.transform(test_data['Sex'])\ntitle_numerical = le_title.transform(test_data['Title'])\ncabin_cat_numerical = le_cabin_cat.transform(test_data['Cabin_cat'])\nembarked_numerical = le_embarked.transform(test_data['Embarked'])\n\n# One-Hot encoding\nsex_onehot = enc_sex.transform(sex_numerical.reshape(-1,1))\ntitle_onehot = enc_title.transform(title_numerical.reshape(-1,1))\ncabin_cat_onehot = enc_cabin_cat.transform(cabin_cat_numerical.reshape(-1,1))\nembarked_onehot = enc_embarked.transform(embarked_numerical.reshape(-1,1))\n\n# Add One-Hot labels to DataFrame\ntest_data = pdAssignWithOHLabel(test_data, 'Sex', \n                                 sex_onehot, sex_numerical_classes)\ntest_data = test_data.drop('Sex',axis=1)\ntest_data = pdAssignWithOHLabel(test_data, 'Title', \n                                 title_onehot, title_numerical_classes)\ntest_data = test_data.drop('Title',axis=1)\ntest_data = pdAssignWithOHLabel(test_data, 'Cabin_cat', \n                            cabin_cat_onehot, cabin_cat_numerical_classes)\ntest_data = test_data.drop('Cabin_cat',axis=1)\ntest_data = pdAssignWithOHLabel(test_data, 'Embarked', \n                            embarked_onehot, embarked_numerical_classes)\ntest_data = test_data.drop('Embarked',axis=1)\n\n# Impute missing data in all features\n# Add training and test data together, to more accurately find nearest neighbors\nall_data = train_data_df.drop('Survived',axis=1).append(test_data)\n\n# Define updated knnImpute function:\ndef knnImputeTest(test_arr, all_arr, tmp_imp_arr, feature, k=6): # improved one\n    from sklearn.neighbors import KDTree\n    row_idx = test_arr[pd.isnull(test_arr[feature])].index.tolist()\n    tree = KDTree(tmp_imp_arr.values) # tmp_imp_arr is the scaled array without \n                                      # the null-containing feature\n    #row_idx = np.add(row_idx, -1)\n    for nan_v in row_idx:\n        dist, ind = tree.query(tmp_imp_arr.loc[nan_v].values.reshape(1,-1), k)\n        nn_vals = all_arr[feature].loc[ind[0][1:]]\n        imp_val = np.floor(np.nanmean(nn_vals))+0.5 \n        # Per the documentation on this Kaggle data set, estimated\n        # 'Age' values are of the form x.5\n        test_arr = test_arr.set_value(nan_v, feature, imp_val)\n        all_arr = all_arr.set_value(nan_v, feature, imp_val)\n    return test_arr, all_arr\n\nfeats = []\nfor feat in all_data.columns:\n    feats.append(feat)\nneed_imp = np.empty([0,2])\nfor feat in feats:\n    if pd.isnull(all_data[feat]).sum() > 0:\n        need_imp = np.append(need_imp, np.array([[feat,pd.isnull(all_data[feat]).sum()]]), axis=0)\n\n# Sort features by number of imputations that need to be performed\nsort_idx = need_imp[:,1].argsort()\nneed_imp = need_imp[sort_idx]\n\nfor ii in range(len(need_imp)):\n    tmp = all_data = train_data_df.copy().drop('Survived',axis=1).append(test_data)\n    tmp = tmp.drop(list(need_imp[ii:,0]), axis=1)\n    sc = StandardScaler()\n    tmp_scaled =  pd.DataFrame(sc.fit_transform(tmp), columns=tmp.columns, index = tmp.index)\n    test_data, all_data = knnImputeTest(test_data, all_data, tmp_scaled, need_imp[ii,0], 11)\n\n# Check if imputation was done correctly:\nif pd.isnull(test_data).sum().sum() > 0:\n    raise ImputeError('{} NaNs in the data, so data was not imputed correctly'.format(\n        pd.isnull(test_data).sum().sum()))\n\n# Make sure column order is the same as in the training data, so that scaling can be performed\ncolumns = ['Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male',\n       'Title_Army.', 'Title_Dr.', 'Title_Master.', 'Title_Miss.', 'Title_Mr.',\n       'Title_Mrs.', 'Title_Noble.', 'Title_Rev.', 'Cabin_cat_A',\n       'Cabin_cat_B', 'Cabin_cat_C', 'Cabin_cat_D', 'Cabin_cat_E',\n       'Cabin_cat_F', 'Cabin_cat_G', 'Cabin_cat_None', 'Cabin_cat_T',\n       'Embarked_C', 'Embarked_Q', 'Embarked_S']\ntest_data = test_data[columns]\n    \n# Scale the test data using the StandardScaler that was fit on the original training data,\n# 'sc_training'\ntmp = test_data.copy()\ntmp = pd.DataFrame(sc_training.transform(tmp),columns=test_data.columns, index=test_data.index)\ntest_data_scaled = tmp",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Predicting test samples",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Classify the test data with best classifier\npreds = np.empty([test_data_scaled.values.shape[0],1])\npreds = best_clf_retrained.predict(test_data_scaled)\npreds = np.round(preds).astype('int')\n\n# Write predictions to csv file\npreds_csv = pd.DataFrame(test_data.index.values, columns=['PassengerId'])\npreds_csv = preds_csv.assign(Survived=preds)\ncsv_str = 'preds_'+timestr+'.csv'\npreds_csv.to_csv(csv_str, index=False)\n\npreds_csv",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}